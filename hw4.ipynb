{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISE 633 Homework 4 (coding part)\n",
    "\n",
    "Author: Yue Wu <wu.yue@usc.edu>\n",
    "$\\def\\vf#1{\\boldsymbol{#1}}$\n",
    "\n",
    "This notebook is the coding part of the 4th homework of ISE 633. It is written in a style where code and explanations are interleaved. The notebook is written in Python 3.12 and is best viewed in a Jupyter notebook environment.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "It requires the following packages:\n",
    "\n",
    "```plain\n",
    "jax==0.4.25\n",
    "jaxtyping==0.2.25\n",
    "beartype==0.17.2\n",
    "equinox==0.11.3\n",
    "```\n",
    "\n",
    "and reasonably new versions of `ipytest`, `seaborn`, `matplotlib` and `pandas`. The notebook is exclusively written in Python 3.12+. Since the computation is done with JAX with its fully deterministic RNGs, the results should be completely reproducible. The local dependencies (i.e. `utils.*`) will be available in the same directory as this notebook in my github [repository](https://github.com/EtaoinWu/ise633).\n",
    "\n",
    "A significant portion of code in this notebook is identical to my previous homeworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import beartype\n",
    "import equinox as eqx\n",
    "import ipytest\n",
    "import jax\n",
    "import jax.scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from beartype.typing import Any, Callable, cast\n",
    "from jax import numpy as jnp, random as jr, tree_util as jtu\n",
    "from jaxtyping import Array, Bool, Float, Integer, Key, Scalar, jaxtyped\n",
    "from matplotlib.axes import Axes\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import utils.platform\n",
    "from utils.tree import tree_nfold_cross_validation_tests, tree_nfold_cross_validation_trains\n",
    "\n",
    "ipytest.autoconfig()\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_threefry_partitionable\", True)\n",
    "\n",
    "utils.platform.init_matplotlib(\"svg\")\n",
    "sns.set_theme(\"notebook\", style=\"whitegrid\")\n",
    "\n",
    "SingleKey = Key[Scalar, \"\"]\n",
    "typechecked = jaxtyped(typechecker=beartype.beartype)\n",
    "FloatLike = float | Float[Scalar, \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define our linear regression model and target function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressions(eqx.Module):\n",
    "    X: Float[Array, \"m d\"]\n",
    "    y: Float[Array, \" m\"]\n",
    "\n",
    "    @typechecked\n",
    "    def __call__(self, w: Float[Array, \" d\"], key: SingleKey | None = None) -> Float[Scalar, \"\"]:\n",
    "        \"\"\"\n",
    "        Compute the loss of a predictor.\n",
    "        If `key` is supplied, the function is a unbiased estimator of the true loss.\n",
    "        \"\"\"\n",
    "        def inner(w: Float[Array, \" d\"], x: Float[Array, \" d\"], y: Float[Scalar, \"\"]) -> Float[Scalar, \"\"]:\n",
    "            wx = jnp.dot(w, x)\n",
    "            return jnp.log(1 + jnp.exp(wx)) - y * wx\n",
    "        if key is None:\n",
    "            return jnp.mean(jax.vmap(partial(inner, w))(self.X, self.y))\n",
    "        else:\n",
    "            random_index = jr.randint(key, (1,), 0, self.X.shape[0])\n",
    "            return inner(w, self.X[random_index], self.y[random_index])\n",
    "        \n",
    "    @typechecked\n",
    "    def accuracy(self, w: Float[Array, \" d\"]) -> Float[Scalar, \"\"]:\n",
    "        \"\"\"\n",
    "        Compute the accuracy of a predictor.\n",
    "        \"\"\"\n",
    "        predicted = jnp.sign(jnp.dot(self.X, w)) * 0.5 + 0.5\n",
    "        return 1 - jnp.mean(jnp.abs(predicted - self.y))\n",
    "\n",
    "class LinearRegression(LinearRegressions):\n",
    "    pass\n",
    "\n",
    "@typechecked\n",
    "def load_regression(m: int, d: int, x_file: str, y_file: str) -> LinearRegression:\n",
    "    \"\"\"\n",
    "    Load a regression problem from files.\n",
    "    \"\"\"\n",
    "    X = jnp.asarray(np.loadtxt(x_file, delimiter=\",\"))\n",
    "    y = jnp.asarray(np.loadtxt(y_file))\n",
    "    assert X.shape == (m, d)\n",
    "    assert y.shape == (m,)\n",
    "    return LinearRegression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_instance = load_regression(2000, 1000, \"hw4/X.txt\", \"hw4/y.txt\")\n",
    "lr_instance.X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of algorithms\n",
    "\n",
    "Implement proximal gradient and sub-gradient method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepSizeType = FloatLike | Callable[[Integer[Scalar, \"\"]], FloatLike]\n",
    "TargetFn = Callable[[Float[Array, \" d\"]], Float[Scalar, \"\"]]\n",
    "RegularizerFn = Callable[[Float[Array, \" d\"]], Float[Scalar, \"\"]]\n",
    "ProximalFn = Callable[[Float[Array, \" d\"], Float[Scalar, \"\"]], Float[Array, \" d\"]]\n",
    "\n",
    "def l1_regularizer(w: Float[Array, \" d\"]) -> Float[Scalar, \"\"]:\n",
    "    return jnp.sum(jnp.abs(w))\n",
    "\n",
    "def l1_prox(w: Float[Array, \" d\"], weight: Float[Scalar, \"\"]) -> Float[Array, \" d\"]:\n",
    "    return jnp.sign(w) * jnp.maximum(jnp.abs(w) - weight, 0)\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def subgradient_step(\n",
    "    x: Float[Array, \" d\"],\n",
    "    fun: TargetFn,\n",
    "    extra: None | Integer[Scalar, \"\"] = None,\n",
    "    /,\n",
    "    *,\n",
    "    regularizer: RegularizerFn,\n",
    "    weight: FloatLike,\n",
    "    stepsize: StepSizeType,\n",
    ") -> tuple[Float[Array, \" d\"], Integer[Scalar, \"\"]]:\n",
    "    t = jnp.array(1, dtype=int) if extra is None else extra\n",
    "    def regularized_fun(w: Float[Array, \" d\"]) -> Float[Scalar, \"\"]:\n",
    "        return fun(w) + weight * regularizer(w)\n",
    "    direc = -jax.grad(regularized_fun)(x)\n",
    "    stepsize = stepsize(t) if callable(stepsize) else stepsize\n",
    "    return (x + stepsize * direc, t + 1)\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def proximal_gradient_step(\n",
    "    x: Float[Array, \" d\"],\n",
    "    fun: TargetFn,\n",
    "    extra: None | Integer[Scalar, \"\"] = None,\n",
    "    /,\n",
    "    *,\n",
    "    regularizer: RegularizerFn,\n",
    "    weight: FloatLike,\n",
    "    prox: ProximalFn,\n",
    "    stepsize: StepSizeType,\n",
    ") -> tuple[Float[Array, \" d\"], Integer[Scalar, \"\"]]:\n",
    "    t = jnp.array(1, dtype=int) if extra is None else extra\n",
    "    def regularized_fun(w: Float[Array, \" d\"]) -> Float[Scalar, \"\"]:\n",
    "        return fun(w) + weight * regularizer(w)\n",
    "    direc = -jax.grad(regularized_fun)(x)\n",
    "    stepsize = stepsize(t) if callable(stepsize) else stepsize\n",
    "    return (prox(x + stepsize * direc, weight * stepsize), t + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main descent framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extra = Any | None\n",
    "StepFunction = Callable[\n",
    "    [\n",
    "        Float[Array, \" n\"],\n",
    "        TargetFn,\n",
    "        Extra,\n",
    "    ],\n",
    "    Float[Array, \" n\"] | tuple[Float[Array, \" n\"], Extra],\n",
    "]\n",
    "Carry = tuple[Float[Array, \" n\"], Extra]\n",
    "Output = tuple[Float[Array, \" n\"], Float[Scalar, \"\"]]\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=[2], static_argnames=[\"max_iter\"])\n",
    "@typechecked\n",
    "def descent(\n",
    "    x0: Float[Array, \" d\"],\n",
    "    fun: TargetFn,\n",
    "    step: StepFunction,\n",
    "    /,\n",
    "    max_iter: int = 100,\n",
    "    extra0: Extra = None,\n",
    ") -> tuple[Float[Array, \"m n\"], Float[Scalar, \" m\"]]:\n",
    "    @jax.jit\n",
    "    @typechecked\n",
    "    def iter(carry: Carry, input: None) -> tuple[Carry, Output]:\n",
    "        x, extra = carry\n",
    "        new = step(x, fun, extra)\n",
    "        if isinstance(new, tuple):\n",
    "            new_x, new_extra = new\n",
    "        else:\n",
    "            new_x, new_extra = new, extra\n",
    "        return (new_x, new_extra), (x, fun(x))\n",
    "\n",
    "    (x1, extra1), (x0, f0) = iter((x0, extra0), None)\n",
    "    cn, os = jax.lax.scan(iter, (x1, extra1), None, length=max_iter - 1)\n",
    "    xs, fs = os\n",
    "    return jnp.concatenate([x0[None, :], xs]), jnp.concatenate(\n",
    "        [f0[None], fs]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(\n",
    "    path: Float[Array, \"m n\"],\n",
    "    losses: Float[Scalar, \" m\"],\n",
    "    weight: FloatLike,\n",
    "    *,\n",
    "    ax: Axes | None = None,\n",
    "    stride: int = 10,\n",
    ") -> None:\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    it = jnp.arange(path.shape[0])\n",
    "    path_diff = jnp.diff(path, axis=0)\n",
    "    path_diff_len = jnp.linalg.norm(path_diff, axis=1)\n",
    "    ax_step = ax\n",
    "    ax_loss: Axes = ax.twinx()\n",
    "    color_step, color_loss = sns.color_palette(\"deep\", 2)\n",
    "    ax_step.plot(\n",
    "        it[:-1:stride], path_diff_len[::stride], color=color_step\n",
    "    )\n",
    "    ax_step.set_ylabel(\"Step size\", color=color_step)\n",
    "    ax_step.tick_params(axis=\"y\", which=\"both\", labelcolor=color_step)\n",
    "    ax_step.set_yscale(\"log\")\n",
    "\n",
    "    ax_loss.plot(it[::stride], losses[::stride], color=color_loss)\n",
    "    ax_loss.set_ylabel(\"Loss\", color=color_loss)\n",
    "    ax_loss.tick_params(axis=\"y\", which=\"both\", labelcolor=color_loss)\n",
    "    ax_loss.set_yscale(\"log\")\n",
    "\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_title(f\"Weight: {weight:.5f}\")\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"max_iter\"])\n",
    "def weight_tune_test(\n",
    "    weight: FloatLike,\n",
    "    training: LinearRegression,\n",
    "    test: LinearRegression,\n",
    "    max_iter: int = 1000,\n",
    ") -> Float[Scalar, \"\"]:\n",
    "    step = partial(\n",
    "        proximal_gradient_step,\n",
    "        regularizer=l1_regularizer,\n",
    "        weight=weight,\n",
    "        prox=l1_prox,\n",
    "        stepsize=1e-3,\n",
    "    )\n",
    "    path, losses = descent(\n",
    "        jnp.zeros(training.X.shape[1]),\n",
    "        training,\n",
    "        step,\n",
    "        max_iter=max_iter,\n",
    "    )\n",
    "\n",
    "    return test.accuracy(path[-1]), path, losses\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"n\", \"max_iter\"])\n",
    "@typechecked\n",
    "def nfold_weight_tune_test(\n",
    "    n: int,\n",
    "    weight: FloatLike,\n",
    "    training: LinearRegression,\n",
    "    max_iter: int = 1000,\n",
    ") -> Float[Scalar, \"\"]:\n",
    "    trainings, tests = (\n",
    "        tree_nfold_cross_validation_trains(n, training),\n",
    "        tree_nfold_cross_validation_tests(n, training),\n",
    "    )\n",
    "    return jnp.mean(\n",
    "        jax.vmap(\n",
    "            lambda tr, te: weight_tune_test(weight, tr, te, max_iter)[0]\n",
    "        )(trainings, tests)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to test the basic functionalities of weight tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = jtu.tree_map(lambda x: x[:200], lr_instance)\n",
    "test = jtu.tree_map(lambda x: x[1500:], lr_instance)\n",
    "\n",
    "fig, axes_ = plt.subplots(2, 2, figsize=(7, 7), layout=\"constrained\")\n",
    "axes = cast(list[Axes], axes_.flatten())\n",
    "min_weight, max_weight = 1e-5, 1e-3\n",
    "best_weight = 0\n",
    "best_accuracy = 0\n",
    "for i, ax in enumerate(axes):\n",
    "    i_normalized = i / len(axes)\n",
    "    weight = min_weight * (max_weight / min_weight) ** i_normalized\n",
    "    accuracy, path, losses = weight_tune_test(weight, training, test, max_iter=4000)\n",
    "    losses = losses + weight * jax.vmap(l1_regularizer)(path)\n",
    "    visualize(path, losses, weight, ax=ax)\n",
    "    print(f\"Weight: {weight:.7f}, Accuracy: {accuracy:.3f}\")\n",
    "    if accuracy > best_accuracy:\n",
    "        best_weight = weight\n",
    "        best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = jtu.tree_map(lambda x: x[:200], lr_instance)\n",
    "test = jtu.tree_map(lambda x: x[1500:], lr_instance)\n",
    "\n",
    "min_weight, max_weight = 1e-5, 0.3\n",
    "weights = jnp.exp(jnp.linspace(jnp.log(min_weight), jnp.log(max_weight), 100))\n",
    "best_accuracy, best_weight = 0, 0\n",
    "for weight in tqdm.tqdm(weights):\n",
    "    accuracy = nfold_weight_tune_test(5, weight, training, max_iter=5000)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_weight = weight\n",
    "        best_accuracy = accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
